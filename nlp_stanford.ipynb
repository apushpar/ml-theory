{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html\n",
    "## Youtube playlist: https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP Hard:\n",
    "- \"Crash blossoms\" - Syntactic Ambguity: a sentence may be interpreted in more than one way due to ambiguous sentence structure\n",
    "- Ambiguity is pervasive\n",
    "- Non-standard english\n",
    "- Segmentation issues\n",
    "- Idioms: dark horse, get cold feet\n",
    "- Neologisms (new words): unfriend, retweet, bromance\n",
    "- Tricky entity names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical NLP approach\n",
    "- Viterbi\n",
    "- Naive bayes, maxent classifiers\n",
    "- N-gram language model\n",
    "- Statistical parsing\n",
    "- Inverted index, tf-idf, vector models of meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text processing\n",
    "### Regular expressions\n",
    "- Regular expressions: Sophisticated sequences of regular expressions are often the first model for any text processing text\n",
    "- Regular expressions also used as features in ML classifiers\n",
    "- Can be useful in capturing generalizations\n",
    "\n",
    "### Word tokenization\n",
    "#### Text Normalization\n",
    "- Segmenting/tokenizing words\n",
    "- Normalizing word formats\n",
    "\n",
    "### __Token__ : # of words\n",
    "\n",
    "### __Lemma__ : Same stem, part of speech, rough word sense\n",
    "- __cat__ and __cats__ = same lemma\n",
    "\n",
    "### __Wordform__ : the full inflected surface form\n",
    "- __cat__ and __cats__ = different workforms\n",
    "\n",
    "### Standard notations\n",
    "- N = Number of tokens\n",
    "- V = Vocabulary = set of types\n",
    "- |V| is the size of the vocabulary\n",
    "- eg: Shakespeare has Tokens = N = 884,000 and Types = |V| = 31 thousand\n",
    "- Google N-grams N = 1 trillion, |V| = 12 million\n",
    "\n",
    "***\n",
    "*code to find the words counts from shakespeare* <br>\n",
    "tr 'A-Z' 'a-z' < t8.shakespeare.txt | tr -sc 'A-za-z' '\\n' | sort | uniq -c | sort -n -r | less\n",
    "***\n",
    "\n",
    "### Issues in Tokenization\n",
    "- Finland's captial -> Finland Finlands Finland's??\n",
    "- What're, I'm, isn't -> What are, I am, is not\n",
    "- Hewlett-Packard -> Hewlett Packard?\n",
    "- state-of-the-art -> state of the art?\n",
    "- Lowercase -> lower-case lowercase lower case?\n",
    "- San Francisco -> one token or two?\n",
    "- m.p.h., PhD. -> ?? \n",
    "- Other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Matching Word segmentation Algorithm\n",
    "- Given a wordlist of Chinese and a string.\n",
    "> 1. Start a pointer at the beginning of the string\n",
    "> 2. Find the longest word in dict that macthes the string starting at pointer\n",
    "> 3. Move the pointer over the word in string\n",
    "> 4. Go to 2\n",
    "- thecatinthehat -> start the \"th...\" longest word is the..next start with c...longest word is cat..-> the cat in the hat\n",
    "- This fails for Thetabledownthere -> should be \"The table down there\" instead we get \"theta bled own there\"\n",
    "- This algo doesnt work well for English\n",
    "_ This works well for chinese since chinese has almost consistent word lengths (whereas english has long and short words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Normalization and Stemming\n",
    "#### Normalization\n",
    "- Normalization eg: U.S.A to USA\n",
    "- Use lowercase (exception: distinguish between verb and noun)\n",
    "- Case is useful in sentiment analysis\n",
    "#### Lemmatization\n",
    "- Reduce inflections or variant forms to base form\n",
    "> - am, are, is -> be\n",
    "> - car, cars, car's, cars' -> car\n",
    "- Find correct dictionary headword form\n",
    "- __Stemming__: Reduce terms to their stems. It's crude chopping of affixes\n",
    "\n",
    "#### Porter's algo for stemming: Most common [Porter Stemmer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segementation\n",
    "- !.? are relatively unambiguous\n",
    "- period \".\" is quite ambiguous\n",
    "> - sentence boundary\n",
    "> - Abbreviations like Dr.\n",
    "> - Float\n",
    "- Solution: __Binary Classifier__ : End of sentence or not end of sentence\n",
    "> - Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance\n",
    "- Solving string similarity\n",
    "- defn: Min. # of edit operations needed to transform one word into the other\n",
    "- Use case: Spell correction\n",
    "- Another use case: Evaluating machine translation and speech recognition\n",
    "- Useful in Name Entity Extraction\n",
    "- Algo (Dynamic programming)\n",
    "> D(i,j) = min(D(i-1,j)+1 , D(i, j-1)+1, D(i-1, j-1)+2?if S[i]!=T[j] else 0)\n",
    "\n",
    "### Weighted Min edit distance\n",
    "- Not all alphabets are mis-spelled in the same way\n",
    "- Give some alphabets more weights and use that in the insert/del/sub operations to calculate the edit distance\n",
    "- Lookup tables to find the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
